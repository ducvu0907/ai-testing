{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58f89adc-1871-4821-af95-684e3ac99ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f66d37380b0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9fefeb2-b07d-43da-af2f-f2a3cdbd0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3820d768-4866-4ab4-b2b4-37ea6a2d13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"alice.txt\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b4fbd2c-d86f-4644-a5d1-b4c297caa894",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b82053d0-11e9-4c06-a6fd-476b6b943b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26683"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42a4bdd9-78fb-409a-8cf4-0d4798d86565",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)\n",
    "word_to_idx = {word:i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8758f44f-1b75-4a4f-a3bc-c5143819d147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2854"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52591193-3a49-429f-88fb-6e8bf576c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramWithSoftmax(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embedding):\n",
    "        super(SkipGramWithSoftmax, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, n_embedding)\n",
    "        self.embedding_context = nn.Embedding(vocab_size, n_embedding)\n",
    "        nn.init.uniform_(self.embedding_center.weight, -1, 1)\n",
    "        nn.init.uniform_(self.embedding_context.weight, -1, 1)\n",
    "    def forward(self, center_word):\n",
    "        emb_center = self.embedding_center(center_word)\n",
    "        out = torch.matmul(emb_center, self.embedding_context.weight.T) # logits for every word in the corpus\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24b6349a-8e77-486f-b3ff-8350bf433589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(words, vocab, context_size):\n",
    "    x, y = [], []\n",
    "    for i, word in enumerate(words):\n",
    "        l = max(0, i - context_size)\n",
    "        r = min(len(words) - 1, i + context_size)\n",
    "        for j in range(l, r + 1):\n",
    "            if i != j:\n",
    "                x.append(word_to_idx[word])\n",
    "                y.append(word_to_idx[words[j]])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a6c1423-87cd-421f-9496-25983232c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data(words, vocab, context_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58d51d4f-adf2-434b-acb1-502c79035953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106726, 106726)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "edb10ccd-1eee-468d-a8ae-7f322a32c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "n_embedding = 50\n",
    "model = SkipGramWithSoftmax(vocab_size, n_embedding)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ade19de-fe0a-4280-b5d5-bdb99ccb7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, center_words, context_words):\n",
    "        self.center_words = center_words\n",
    "        self.context_words = context_words\n",
    "    def __len__(self):\n",
    "        return len(self.center_words)\n",
    "    def __getitem__(self, index):\n",
    "        return self.center_words[index], self.context_words[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c01cbd4d-0a6c-45e2-a53c-56994766ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a2310ba-c1b1-4a66-b48d-edfca850bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19051193-2356-44b3-96c0-cfbed61fa3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8565.318466186523\n",
      "8473.746932983398\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
